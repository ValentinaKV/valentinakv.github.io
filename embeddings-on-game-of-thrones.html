<!DOCTYPE html>
<html lang="en">

<head>
      <meta charset="utf-8">
  <meta http-equiv="Content-Type" content="text/html" charset="UTF-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />


  <title>Embeddings on Game of Thrones</title>


  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="referrer" content="origin" />
  <meta name="generator" content="Pelican" />
  <link href="" rel="canonical" />

  <!-- Feed -->

  <link href="/theme/css/style.css" type="text/css" rel="stylesheet" />

  <!-- Code highlight color scheme -->
      <link href="/theme/css/code_blocks/github.css" rel="stylesheet">


  <!-- Custom fonts -->
  <link href='https://fonts.googleapis.com/css?family=Montserrat:400,300' rel='stylesheet' type='text/css' />
  <link href="https://fonts.googleapis.com/css?family=Lato" rel="stylesheet" type="text/css" />

  <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
  <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
  <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
    <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
  <![endif]-->


    <link href="/embeddings-on-game-of-thrones.html" rel="canonical" />

        <meta name="description" content="Embedding One of the key tasks of natural language processing is word embedding. As machine learning algorithms require numerical input,...">

        <meta name="author" content="nova">


        <meta property="og:locale" content="en_US" />
    <meta property="og:site_name" content="DEV" />
    <meta property="og:type" content="website" />
    <meta property="og:title" content="DEV" />
    <meta property="og:description" content="View the blog." />
    <meta property="og:url" content="" />
      <meta name="og:image" content="/theme/images/post-bg.jpg">

  <meta property="og:type" content="article">
            <meta property="article:author" content="/author/nova.html">
  <meta property="og:url" content="/embeddings-on-game-of-thrones.html">
  <meta property="og:title" content="Embeddings on Game of Thrones">
  <meta property="article:published_time" content="2021-01-17 00:00:00+01:00">
            <meta property="og:description" content="Embedding One of the key tasks of natural language processing is word embedding. As machine learning algorithms require numerical input,...">

            <meta property="og:image" content="/theme/images/post-bg.jpg">
</head>
<!-- TODO : Body class -->
<body class="home-template">

<nav id="menu">
  <a class="close-button">Close</a>
  <div class="nav-wrapper">
    <p class="nav-label">Menu</p>
    <ul>


    </ul>
  </div>
</nav>
    <!-- Progressbar -->
    <div class="progress-container">
        <span class="progress-bar"></span>
    </div>

    <!-- Page Header -->
    <!-- Set your background image for this header on the line below. -->
    <header id="post-header" class="has-cover">
      <div class="inner">
        <nav id="navigation">
            <span id="home-button" class="nav-button">
                <a class="home-button" href="" title="Home"><i class="ic ic-arrow-left"></i> Home</a>
            </span>
          <span id="menu-button" class="nav-button">
            <a class="menu-button"><i class="ic ic-menu"></i> Menu</a>
          </span>
        </nav>
        <h1 class="post-title">Embeddings on Game of Thrones</h1>
        <!-- TODO : Proper class for headline -->
        <span class="post-meta">
                <a href="/author/nova.html">Nova</a>
            | <time datetime="Sun 17 January 2021">Sun 17 January 2021</time>
        </span>
        <!-- TODO : Modified check -->
        
            <div class="post-cover cover" style="background-image: url('/theme/images/post-bg.jpg')">
        
      </div>
    </header>    

  <section id="wrapper">
    <a class="hidden-close"></a>

    <!-- Post content -->
    <main class="content" role="main">
        <article class="post">
        <div class="inner">
            <section class="post-content">
                <h2>Embedding</h2>
<p>One of the key tasks of natural language processing is <strong>word embedding</strong>. As machine learning algorithms require numerical input, text data should be converted to meaningful vectors, which is called word embeddings. There are two ways to obtain such embeddings:</p>
<ul>
<li>Use pre-trained vectors such as GloVe or Bert. </li>
<li>Train embeddings yourself. </li>
</ul>
<p>If you choose the second option, you should decide whether you will train embeddings separately from the main (e.g. one-hot encoding, word2vec) or during the model training (using Embedding layer in Neural Networks, which anyway requires one-hot encoding).
So <strong>one-hot encoding</strong> is the “simplest” way of encoding. Result of this method is N (number of input samples) encoded vectors with length equal to the size of the vocabulary. Vocabulary is a list of unique words.
For example, for sentences “It is a number” and “It is a text” the vocabulary is <code>[a, is, it, number, text]</code> and one-hot encoded vectors are: “It is a number” - <code>[1, 1, 1, 1, 0]</code>;  “It is a text” - <code>[1, 1, 1, 0, 1]</code>. But in practice these vectors can become enormous because of the huge vocabulary size. Moreover the difference or similarity of these vectors doesn’t represent their lexical difference or similarity.
Therefore, there are other algorithms of word representation, which provide similar representation to similar words.</p>
<h2>Word2vec</h2>
<p><strong>Word2vec</strong> trains vectors depending on words co-occurrence: if some words often take place in the same context, they are located closer to each other in the vector space. The architecture of word2vec is similar to encoders and it works in one of the two ways: predict a word using a context (continuous bag of words - CBOW) or predict a context using a word (skip-gram). The results of the algorithm were almost magical at that moment, what is shown in the famous example in the original paper:  <strong>king - man + woman = queen</strong>. </p>
<p>Nevertheless, after this paper a lot of other “better” algorithms (like BERT and then GPT) have appeared. They use different architecture (like transformers) and provide higher accuracy. We’ll not go into the details of other models, but we’re going to try word2vec on practice. Let’s train embeddings on the Game of Thrones book using gensim library.</p>
<h2>Game of Thrones</h2>
<p>The famous book contains three main storylines: the Seven Kingdom and the queen; the Wall and the Starks; and Khal Drogo and the Narrow Sea. Using the trained model I’ve obtained the 20 most similar words of the following topics: queen, wall, dothraki and got a quite good result. There are some notable words among the “associations” of the model: 
Queen: cersei, myrcella, joff, tommen, utterly, concern
Wall: eastwatch, kingsroad, walking, wolfswood, battlements, wildlings, northern, northerness
Dothrak: vaes, warriors, khal, bloodrider, handmaids</p>
<p>We can visualize PCA components to get some vision of how those words relate to each other  within the groups and how the groups  are located in two-dimensional space. <img alt="PCA visualization" src="images/scatter.png"></p>
<p>This scatter plot doesn’t say much about the data as it shows PCA components, but we can conclude the groups are distinguishable. The lists of the closest words includes some redundant words (words in plural, adverbs and other), we can discard or get base form of such kinds of words, which depends on the final goal.
We can improve the results by doing more advanced preprocessing and providing deeper research to adjust model’s parameters, which are more relevant for “understanding” the book’s characters and their stories.
The code is available on <a href="https://github.com/ValentinaKV/embeddings">GitHub</a>. Feel free to open pull requests.</p>
            </section>

            <section class="post-info">
                <div class="post-share">
                    <a class="twitter" href="https://twitter.com/share?text=Embeddings on Game of Thrones&amp;url=/embeddings-on-game-of-thrones.html" onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
                    <i class="ic ic-twitter"></i><span class="hidden">Twitter</span>
                    </a>
                    <a class="facebook" href="https://www.facebook.com/sharer/sharer.php?u=/embeddings-on-game-of-thrones.html" onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;">
                    <i class="ic ic-facebook"></i><span class="hidden">Facebook</span>
                    </a>
                    <a class="googleplus" href="https://plus.google.com/share?url=/embeddings-on-game-of-thrones.html" onclick="window.open(this.href, 'google-plus-share', 'width=490,height=530');return false;">
                    <i class="ic ic-googleplus"></i><span class="hidden">Google+</span>
                    </a>
                    <div class="clear"></div>
                </div>


                <div class="clear"></div>

 

                </section>


                <aside class="post-nav">
                    <div class="clear"></div>
                </aside>

            </div>
        </article>
    </main>
      <!-- TODO : Body class -->
    <div id="body-class" style="display: none;" class=""></div>
  
    <footer id="footer">
      <div class="inner">
        <section class="credits">
          <span class="credits-theme">Theme <a href="https://github.com/arulrajnet/attila" rel="nofollow">Attila</a></span>
          <span class="credits-software">Published with <a href="https://github.com/getpelican/pelican" rel="nofollow">Pelican</a></span>
        </section>
      </div>
    </footer>
  </section>

  <script type="text/javascript" src="/theme/js/script.js"></script>
  
</body>
</html>